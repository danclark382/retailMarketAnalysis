{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import regex\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spaczz.matcher import FuzzyMatcher\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dclark171/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (1,2,10,11,20,23,35,63,71,72,77,84,85,86,87,89,90,91,92,93,94,95,97,100,101,103,104,105,106,108,110) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/Users/dclark171/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "df = pd.read_csv(\"/Users/dclark171/projects/python/retailMarketAnalysis/data/reddit_submissions/all_submissions.csv\")\n",
    "sample = df.dropna(subset=['selftext'])[df['selftext'] != '[removed]'][df['selftext'] != '[deleted]'].sample(1000)\n",
    "\n",
    "with open(\"/Users/dclark171/projects/python/retailMarketAnalysis/data/iex_data.json\", 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "with open('/Users/dclark171/projects/python/retailMarketAnalysis/data/english_dictionary.txt') as word_file:\n",
    "    valid_words = set(word_file.read().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00,  5.33it/s]/Users/dclark171/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/dclark171/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "5385it [25:06,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#tickers = list(pd.read_csv(\"/Users/dclark171/projects/python/retailMarketAnalysis/data/tickers.csv\")['ticker'])\n",
    "#tentative = []\n",
    "for idx, x in tqdm(enumerate(tickers)):\n",
    "    if x.lower in valid_words or wn.synsets(x):\n",
    "        tentative.append(tickers.pop(idx))\n",
    "    else:\n",
    "        try:\n",
    "            _ = wikipedia.page(x, auto_suggest=False)\n",
    "        except wikipedia.exceptions.DisambiguationError:\n",
    "            tentative.append(tickers.pop(idx))\n",
    "        except wikipedia.exceptions.PageError:\n",
    "            pass\n",
    "print(len(tentative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_company = {}\n",
    "\n",
    "for x in tickers:\n",
    "    if data.get(x):\n",
    "        ticker_company[x] = (data[x]['companyName'], False)\n",
    "\n",
    "for x in tentative:\n",
    "    if data.get(x):\n",
    "        ticker_company[x] = (data[x]['companyName'], True)\n",
    "        \n",
    "with open(\"data/ticker_cmp.json\", 'w') as f:\n",
    "    json.dump(ticker_company, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = dict(filter(lambda s: False if s[1][1] else True, ticker_company.items()))\n",
    "tentative = dict(filter(lambda s: True if s[1][1] else False, ticker_company.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spaczz.matcher import FuzzyMatcher\n",
    "\n",
    "nlp2 = spacy.blank(\"en\")\n",
    "\n",
    "matcher2 = FuzzyMatcher(nlp2.vocab)\n",
    "\n",
    "for k, v in tickers.items():\n",
    "    cmp = v[0]\n",
    "    if cmp:\n",
    "        cmp = re.sub('[^A-Za-z0-9\\s]+|incorporated|ETF|corporation|\" \\\n",
    "                     \"\\sfund|\\scorp|Ltd|\\sinc|\\sco|equity|\\strust|\\sSAA|\\sLP|\\splc|\\s{2,}', '',\n",
    "                     v[0], flags=re.IGNORECASE)\n",
    "        if len(cmp) < 4:\n",
    "            if len(cmp) <= 3:\n",
    "                continue\n",
    "            matcher2.add(k, [nlp2(cmp)], kwargs=[{\"fuzzy_func\": \"simple\", \"min_r1\":30, \"min_r2\": 100}])\n",
    "            continue\n",
    "        matcher2.add(k, [nlp2(cmp)], kwargs=[{\"fuzzy_func\": \"simple\", \"min_r1\":30, \"min_r2\": 90}])\n",
    "for k, v in tentative.items():\n",
    "    cmp = v[0]\n",
    "    if cmp:\n",
    "        cmp = re.sub('[^A-Za-z0-9\\s]+|incorporated|ETF|corporation|\" \\\n",
    "                     \"\\sfund|\\scorp|Ltd|\\sinc|\\sco|equity|\\strust|\\sSAA|\\sLP|\\splc|\\s{2,}', '',\n",
    "                     v[0], flags=re.IGNORECASE)\n",
    "        if len(cmp) < 4:\n",
    "            if len(cmp) <= 3:\n",
    "                continue\n",
    "            matcher2.add(k, [nlp2(cmp)], kwargs=[{\"fuzzy_func\": \"simple\", \"min_r1\":30, \"min_r2\": 100}])\n",
    "            continue\n",
    "        matcher2.add(k, [nlp2(cmp)], kwargs=[{\"fuzzy_func\": \"simple\", \"min_r1\":30, \"min_r2\": 90}])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OFS of capital 95\n",
      "INST structure 90\n",
      "GES guess 100\n",
      "UTL until 91\n",
      "LASR night 91\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-231-0d8ea05cafbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'selftext'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatcher2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/spaczz/matcher/fuzzymatcher.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0mmatches_wo_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmatches_wo_label\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                     matches_w_label = [\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/spaczz/fuzz/fuzzysearcher.py\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(self, doc, query, n, fuzzy_func, min_r1, min_r2, ignore_case, flex)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mflex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calc_flex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mmatch_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scan_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuzzy_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_r1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_case\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmatch_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mpositions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indice_maxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/spaczz/fuzz/fuzzysearcher.py\u001b[0m in \u001b[0;36m_scan_doc\u001b[0;34m(self, doc, query, fuzzy_func, min_r1, ignore_case)\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             match = self.compare(\n\u001b[0;32m--> 374\u001b[0;31m                 \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuzzy_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m             )\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmin_r1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in sample['selftext'][:100]:\n",
    "    doc = nlp2(i)\n",
    "    matches = matcher2(doc)\n",
    "    for match_id, start, end, ratio in matches:\n",
    "        print(match_id, doc[start:end], ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple', 'TICKER'), ('San Francisco', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "nlp = English()\n",
    "ruler = EntityRuler(nlp)\n",
    "patterns = [{\"label\": \"TICKER\", \"pattern\": \"Apple\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}]\n",
    "ruler.add_patterns(patterns)\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "doc = nlp(\"Apple is opening its first big office in San Francisco.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An unidentified flying object (UFO) is any aerial phenomenon that cannot immediately be identified or explained. Most UFOs are identified on investigation as conventional objects or phenomena. The term is widely used for claimed observations of extraterrestrial spacecraft.\n"
     ]
    }
   ],
   "source": [
    "print(wikipedia.summary(\"USO\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/seatgeek/fuzzywuzzy\n",
    "https://pypi.org/project/spaczz/#Fuzzy-Matcher\n",
    "https://spacy.io/usage/rule-based-matching\n",
    "https://spacy.io/usage/rule-based-matching#entityruler\n",
    "https://spacy.io/usage/training    \n",
    "https://medium.com/@b.terryjack/nlp-pretrained-named-entity-recognition-7caa5cd28d7b#:~:text=There%20are%20a%20good%20range,)%20API%20(e.g.%20GATE)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
